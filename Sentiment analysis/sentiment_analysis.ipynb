{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Prediction with Multinomial Naive Bayes\n",
    "\n",
    "In this notebook, we will look at how we can predict the sentiment of a text by training models from scratch.\n",
    "\n",
    "## Contents\n",
    "\n",
    "* [Before we start](#start)\n",
    "* [Importing the data](#data)\n",
    "* [Preprocessing](#preprocessing)\n",
    "* [Vectorization](#vectorization)\n",
    "* [Multinomial Naive Bayes](#mnb)\n",
    "* [Practice](#practice)\n",
    "* [Using another model](#svm)\n",
    "* [A better way to vectorize](#vectorize-better)\n",
    "* [More information](#more-info)\n",
    "\n",
    "## Before we start<a id=\"start\"></a>\n",
    "\n",
    "We will use Pandas and NLTK packages for data manipulation and NLP. In this notebook, you are assumed to know the basics. If your environment does not have the necessary packages or the datasets, you can run this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing necessary packages\n",
    "!pip install pandas \n",
    "!pip install nltk\n",
    "!pip install gensim \n",
    "!pip install contractions\n",
    "!pip install matplotlib\n",
    "!pip install sklearn\n",
    "!pip install numpy as np\n",
    "\n",
    "# Installing some datasets for NLTK:\n",
    "from nltk import download\n",
    "download(\"popular\") # Popular datasets\n",
    "download('tagsets') # Tagsets for POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data<a id=\"data\"></a>\n",
    "\n",
    "We will use the [Sentiment140](https://www.kaggle.com/kazanova/sentiment140) dataset that has 1.6 million tweets, labeled by the existence of positive or negative emoticons. It has its  limitations, but it should suffice for learning purposes. You can check the corresponding paper, [Twitter Sentiment Classification using Distant Supervision](http://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf), for more information. Download the dataset and put it in the same folder with this notebook for convenience. We will use Pandas for data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet_source</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date tweet_source  \\\n",
       "0  0          1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY      \n",
       "1  0          1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY      \n",
       "2  0          1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY      \n",
       "3  0          1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY      \n",
       "4  0          1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY      \n",
       "\n",
       "          username  \\\n",
       "0  _TheSpecialOne_   \n",
       "1  scotthamilton     \n",
       "2  mattycus          \n",
       "3  ElleCTF           \n",
       "4  Karoli            \n",
       "\n",
       "                                                                                                                 tweet  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "1  is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!      \n",
       "2  @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                            \n",
       "3  my whole body feels itchy and like its on fire                                                                       \n",
       "4  @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.       "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# This option will automatically set the column width when we display data:\n",
    "pd.set_option(\"display.max_colwidth\", 0)\n",
    "\n",
    "# Importing it using Pandas. Change the path as necessary. Note that we need to \n",
    "# specify the encoding as \"latin-1\" and we need to provide the headers since the \n",
    "# dataset itself does not have them.\n",
    "dataset = pd.read_csv(\"sentiment140_dataset.csv\", encoding=\"latin-1\", names=[\"sentiment\", \"id\", \"date\", \"tweet_source\", \"username\", \"tweet\"])\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not use most of the columns here, so we can drop them. Also, this dataset's sentiment labels are 0 (negative) and 4 (positive). We can map these to a more familiar range like -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800000</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I LOVE @Health4UandPets u guys r the best!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800001</th>\n",
       "      <td>1.0</td>\n",
       "      <td>im meeting up with one of my besties tonight! Cant wait!!  - GIRL TALK!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800002</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@DaRealSunisaKim Thanks for the Twitter add, Sunisa! I got to meet you once at a HIN show here in the DC area and you were a sweetheart.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800003</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Being sick can be really cheap when it hurts too much to eat real food  Plus, your friends make you soup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800004</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@LovesBrooklyn2 he has that effect on everyone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment  \\\n",
       "0      -1.0         \n",
       "1      -1.0         \n",
       "2      -1.0         \n",
       "3      -1.0         \n",
       "4      -1.0         \n",
       "800000  1.0         \n",
       "800001  1.0         \n",
       "800002  1.0         \n",
       "800003  1.0         \n",
       "800004  1.0         \n",
       "\n",
       "                                                                                                                                            tweet  \n",
       "0       @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D                        \n",
       "1       is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!                            \n",
       "2       @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                                                  \n",
       "3       my whole body feels itchy and like its on fire                                                                                             \n",
       "4       @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.                             \n",
       "800000  I LOVE @Health4UandPets u guys r the best!!                                                                                                \n",
       "800001  im meeting up with one of my besties tonight! Cant wait!!  - GIRL TALK!!                                                                   \n",
       "800002  @DaRealSunisaKim Thanks for the Twitter add, Sunisa! I got to meet you once at a HIN show here in the DC area and you were a sweetheart.   \n",
       "800003  Being sick can be really cheap when it hurts too much to eat real food  Plus, your friends make you soup                                   \n",
       "800004  @LovesBrooklyn2 he has that effect on everyone                                                                                             "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping these columns:\n",
    "dataset.drop(columns=[\"id\", \"date\", \"tweet_source\", \"username\"], inplace=True)\n",
    "\n",
    "# Min-max rescaling:\n",
    "# https://en.wikipedia.org/wiki/Feature_scaling#Rescaling_(min-max_normalization)\n",
    "\n",
    "# These are already known but it is still good practice:\n",
    "dataset_min = min(dataset[\"sentiment\"])\n",
    "dataset_max = max(dataset[\"sentiment\"])\n",
    "\n",
    "# The range we want:\n",
    "new_min = -1\n",
    "new_max = 1\n",
    "\n",
    "# Applying the formula:\n",
    "dataset[\"sentiment\"] = new_min + ((dataset[\"sentiment\"] - dataset_min) * (new_max - new_min) / (dataset_max - dataset_min))\n",
    "\n",
    "# Displaying five rows for both sentiment labels:\n",
    "dataset.groupby(\"sentiment\", as_index=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our dataset is a bit large, it will take some time to process them. If you would like to sample the dataset to speed up the processes, run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraction to drop:\n",
    "frac_to_drop = 0.99\n",
    "\n",
    "# Dropping negative and positive tweets separately to strictly preserve the \n",
    "# ratio (random_state is set to get the same sample every time):\n",
    "dataset = dataset.drop(dataset[dataset.sentiment == -1].sample(frac=frac_to_drop, random_state=42).index)\n",
    "dataset = dataset.drop(dataset[dataset.sentiment == 1].sample(frac=frac_to_drop, random_state=42).index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing<a id=\"preprocessing\"></a>\n",
    "\n",
    "Now, we need to process the tweets before we vectorize them. We will firstly tokenize the tweets into sentences and later words. For each sentence, we will POS-tag the word tokens and lemmatize them using their POS tags. We will also handle negation while removing certain tokens to reduce our vector size. We will use NLTK's tweet tokenizer for word tokenization. The rest of the processes will be handled using NLTK as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>@grum WAH I can't see clip, must be el-stupido work filters. Can't wait 'till I get a 'puter. Something else 2 blame ex 4. He broke mine</td>\n",
       "      <td>[wah, see_NEG, clip_NEG, must_NEG, el-stupido_NEG, work_NEG, filter_NEG, wait_NEG, till_NEG, get_NEG, puter_NEG, something, else, blame, ex, break, mine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>Oh - Just got all my MacHeist 3.0 apps - sweet. Didn't get the Espresso serial no though although they said they sent it - oh well</td>\n",
       "      <td>[oh, get, macheist, apps, sweet, get_NEG, espresso_NEG, serial_NEG, though_NEG, although_NEG, say_NEG, send_NEG, oh_NEG, well_NEG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>I'm gonna get up late tomorrow and it's 132am here. I gonna get tipsy by my lonesome. That's...that's just sad</td>\n",
       "      <td>[go, get, late, tomorrow, 132am, go, get, tipsy, lonesome, sad]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>@Sara_Kate Im afraid too  ( ur reply about uni from ages ago</td>\n",
       "      <td>[afraid, ur, reply, uni, age, ago]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>Wait should I eat?? Or be skinny for vegas!! I'm hungry!</td>\n",
       "      <td>[wait, eat, skinny, vega, hungry]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentiment  \\\n",
       "126 -1.0         \n",
       "159 -1.0         \n",
       "215 -1.0         \n",
       "363 -1.0         \n",
       "390 -1.0         \n",
       "\n",
       "                                                                                                                                         tweet  \\\n",
       "126  @grum WAH I can't see clip, must be el-stupido work filters. Can't wait 'till I get a 'puter. Something else 2 blame ex 4. He broke mine    \n",
       "159  Oh - Just got all my MacHeist 3.0 apps - sweet. Didn't get the Espresso serial no though although they said they sent it - oh well          \n",
       "215  I'm gonna get up late tomorrow and it's 132am here. I gonna get tipsy by my lonesome. That's...that's just sad                              \n",
       "363  @Sara_Kate Im afraid too  ( ur reply about uni from ages ago                                                                                \n",
       "390  Wait should I eat?? Or be skinny for vegas!! I'm hungry!                                                                                    \n",
       "\n",
       "                                                                                                                                               tweet_processed  \n",
       "126  [wah, see_NEG, clip_NEG, must_NEG, el-stupido_NEG, work_NEG, filter_NEG, wait_NEG, till_NEG, get_NEG, puter_NEG, something, else, blame, ex, break, mine]  \n",
       "159  [oh, get, macheist, apps, sweet, get_NEG, espresso_NEG, serial_NEG, though_NEG, although_NEG, say_NEG, send_NEG, oh_NEG, well_NEG]                         \n",
       "215  [go, get, late, tomorrow, 132am, go, get, tipsy, lonesome, sad]                                                                                            \n",
       "363  [afraid, ur, reply, uni, age, ago]                                                                                                                         \n",
       "390  [wait, eat, skinny, vega, hungry]                                                                                                                          "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.data\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from functools import reduce\n",
    "import operator\n",
    "from nltk import pos_tag_sents\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from nltk.corpus import stopwords\n",
    "import contractions\n",
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "# This tokenizer will guess where the sentence ends instead of tokenizing\n",
    "# sentences by simply looking at sentence terminators.\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# This tokenizer will tokenize tweets. \"preserve_case\" parameter can be used to \n",
    "# preserve cases or make it all lowercase. \"reduce_len\" parameter shortens\n",
    "# consecutive character repetitions to at most three consecutive repetitions to\n",
    "# reduce noise.\n",
    "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True)\n",
    "\n",
    "# This function takes the POS tag and returns it in the correct form for\n",
    "# lemmatization:\n",
    "def get_lemmatizer_pos(pos):\n",
    "    pos_start = pos[0] # Takes the first letter to simplify the POS tag\n",
    "    if pos_start == \"J\":\n",
    "        return wn.ADJ\n",
    "    elif pos_start == \"V\":\n",
    "        return wn.VERB\n",
    "    elif pos_start == \"R\":\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return wn.NOUN \n",
    "\n",
    "# This will lemmatize tokens. Note that it assumes the word is a noun unless a\n",
    "# POS tag is provided.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# This retrieves a list of stop words in English, which will be used to remove the \n",
    "# stop words:\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "# These combined punctuations will be used to remove punctuations from tweets (it \n",
    "# is an extension to string.punctuation):\n",
    "punctuations = \"!\\\"“”#$%&'‘’()*+,-./:;<=>?@[\\]^_`{|}~‍\"\n",
    "\n",
    "def preprocess(tweet):\n",
    "    \n",
    "    ###### Contraction expansion\n",
    "    \n",
    "    tweet = contractions.fix(tweet)\n",
    "    \n",
    "    ###### Sentence tokenization\n",
    "\n",
    "    # Separates tweets into sentences:\n",
    "    tweet_sentences = sentence_tokenizer.tokenize(tweet)\n",
    "\n",
    "    ###### Word tokenization\n",
    "\n",
    "    # Tokenization outputs are kept in separate lists for each sentence:\n",
    "    tweet_sentences_tokens = [tokenizer.tokenize(sentence) for sentence in tweet_sentences]\n",
    "\n",
    "    ###### POS-tagging\n",
    "\n",
    "    # POS tagging happens separately for each sentence before they are combined:\n",
    "    tokens_pos = [pos_tag for pos_tags in pos_tag_sents(tweet_sentences_tokens) for pos_tag in pos_tags]\n",
    "\n",
    "    ###### Lemmatization\n",
    "\n",
    "    # For each POS-tagged token, a lemma is obtained:\n",
    "    lemmas = [lemmatizer.lemmatize(token[0], pos=get_lemmatizer_pos(token[1])) for token in tokens_pos]\n",
    "\n",
    "    ###### Negation handling\n",
    "\n",
    "    # Marks negations:\n",
    "    lemmas = mark_negation(lemmas)\n",
    "    \n",
    "    ###### Removals\n",
    "    \n",
    "    # Filters stop words (considers negations):\n",
    "    lemmas = [lemma for lemma in lemmas if not lemma.replace(\"_NEG\", \"\") in stop_words]\n",
    "    \n",
    "    # Filters punctuation (considers negations):\n",
    "    lemmas = [lemma for lemma in lemmas \n",
    "              if not lemma.replace(\"_NEG\", \"\").translate(lemma.maketrans('', '', punctuations)) == \"\"]\n",
    "    \n",
    "    # Filters numbers (considers negation and some other details):\n",
    "    lemmas = [lemma for lemma in lemmas if not lemma.replace(\"_NEG\", \"\").translate(lemma.maketrans({\",\": \"\", \".\": \"\", \"%\": \"\"})).isdigit()]\n",
    "    \n",
    "    # Filters hashtags:\n",
    "    lemmas = [lemma for lemma in lemmas if not lemma.startswith(\"#\")]\n",
    "    \n",
    "    # Filters user handles:\n",
    "    lemmas = [lemma for lemma in lemmas if not lemma.startswith(\"@\")]\n",
    "    \n",
    "    # Filters the lemma by searching for \"https://,\" \"http://,\" or \"www.\" using \n",
    "    # regular expression. If one of them exists, they are not retrieved.\n",
    "    lemmas = [lemma for lemma in lemmas if not re.search(\"(https?:\\/\\/)|(www\\.)\", lemma)]\n",
    "    \n",
    "    return lemmas\n",
    "\n",
    "dataset[\"tweet_processed\"] = dataset[\"tweet\"].map(preprocess)\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now split our dataset into training and testing sets using scikit-learn. Note that we can maintain the class balance by splitting the dataset using `stratify`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset into training and testing (random_state is set to get the \n",
    "# same sample every time):\n",
    "dataset_train, dataset_test = train_test_split(dataset, test_size=0.3, random_state=42, stratify=dataset[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization<a id=\"vectorization\"></a>\n",
    "\n",
    "Now, we have a dataset that we can use to train our model, but we need to represent each tweet in a vector format. Using the training set, let us combine the lemma lists, flatten it, count each lemma's frequency, and look at some statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14775.000000\n",
       "mean     5.123790    \n",
       "std      24.231928   \n",
       "min      1.000000    \n",
       "25%      1.000000    \n",
       "50%      1.000000    \n",
       "75%      2.000000    \n",
       "max      1068.000000 \n",
       "Name: tweet_processed, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = pd.Series(dataset_train.explode('tweet_processed').tweet_processed).value_counts()\n",
    "\n",
    "terms.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we have a highly skewed distribution here as most terms occur only once. If we do not prune our terms, we will end up with a very large vector that mostly corresponds to highly biased terms. To deal with this problem, we can use only the top n terms. Since we have two sentiment groups, we can count these terms separately and select the most frequent terms for each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "go      574\n",
       "get     532\n",
       "miss    313\n",
       "work    308\n",
       "day     308\n",
       "Name: tweet_processed, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top terms from the negative tweets: \n",
    "terms_negative = pd.Series(dataset_train[dataset_train.sentiment == -1].explode('tweet_processed').tweet_processed).value_counts()\n",
    "\n",
    "terms_negative.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get     523\n",
       "go      494\n",
       "good    469\n",
       "love    403\n",
       "day     391\n",
       "Name: tweet_processed, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top terms from the positive tweets: \n",
    "terms_positive = pd.Series(dataset_train[dataset_train.sentiment == 1].explode('tweet_processed').tweet_processed).value_counts()\n",
    "\n",
    "terms_positive.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take the terms that occur more than n times for both groups to obtain the final term list that we will use for training. We can also limit our vector to the top m frequent words from each sentiment to reduce the vector size even more. Depending on your dataset, applying only one of them could be enough. We will simply obtain the top 500 words from each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1st',\n",
       " '<3',\n",
       " 'able_NEG',\n",
       " 'absolutely',\n",
       " 'ache',\n",
       " 'actually',\n",
       " 'add',\n",
       " 'afternoon',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'ah',\n",
       " 'ahh',\n",
       " 'ahhh',\n",
       " 'album',\n",
       " 'almost',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'always',\n",
       " 'amaze']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieving the top 500 terms for both sentiments, remove the duplicates \n",
    "# (since some words occur in both groups), sort them, and obtain a list:\n",
    "vector_terms = sorted(set(terms_positive.nlargest(500).append(terms_negative.nlargest(500)).index))\n",
    "\n",
    "# This would firstly filter the words with a minimum occurrence threshold (4 \n",
    "# in this case), but it makes no difference for this dataset. \n",
    "# vector_terms = sorted(set(terms_positive[terms_positive > 0].nlargest(500).append(terms_negative[terms_negative > 4].nlargest(500)).index))\n",
    "\n",
    "# Vector size:\n",
    "print(\"Vector size:\",len(vector_terms))\n",
    "\n",
    "# The first 20 terms:\n",
    "vector_terms[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a relatively small term list that can represent both groups. We need to represent all tweets according to this term list. To do so, we will vectorize them using the term occurrences.\n",
    "\n",
    "Assume that our term list has only four terms: \"i,\" \"love,\" \"you,\" and \"myself.\" We can represent these terms as a vector (or a one dimensional list in Python): `[\"i\", \"love\", \"you\", \"myself\"]`\n",
    "\n",
    "If a tweet is `Love you.`, when we process it, its vector form would look like this:\n",
    "\n",
    "`[0, 1, 1, 0]`\n",
    "\n",
    "Notice that the first and the last numbers are 0, because the tweet does not have any \"i\" or \"myself\" in it. The first count corresponds to \"i\" while the last one corrsponds to \"myself.\"\n",
    "\n",
    "Therefore, if a tweet is `I love... I love you.`, its vector form would be:\n",
    "\n",
    "`[2, 2, 1, 0]`\n",
    "\n",
    "Since we removed many terms, our vector cannot represent every text perfectly. Even if we included all the terms from our training set, the test set may have many terms that did not exist in our training set, so this is inevitable. If a tweet is `I love you too.`, it would be represented like this:\n",
    "\n",
    "`[1, 1, 1, 0]`\n",
    "\n",
    "This is indistinguishable from `I love you.`, because we cannot represent \"too\" here. When we vectorize the tweets, they simply become bags of words. The only thing we now know about them is how many times some specific words occur in them. Note that we also lose the relationship between the terms and their order. Each word is independent.\n",
    "\n",
    "To programatically vectorize each tweet, we need to create a dictionary that keeps the index of each term from our term list. Then, we can check if a word of a given tweet exists in that dictionary. If so, the dictionary gives us the location to fill in, so that we can obtain the vector. For example, our term index dictionary would be like this:\n",
    "\n",
    "`term_indices = {\"i\": 0, \"love\": 1, \"you\": 2, \"myself\": 3}`\n",
    "\n",
    "Using this, we know which index to fill in for a given word of a given tweet. \n",
    "\n",
    "We start with a list full of zeros: `[0, 0, 0, 0]`\n",
    "\n",
    "For the term \"love,\" we know that its corresponding index is 1 according to our dictionary. Therefore, whenever we see it occur, we increase the number that corresponds to its index. Let us see how it works in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1st', 0),\n",
       " ('<3', 1),\n",
       " ('able_NEG', 2),\n",
       " ('absolutely', 3),\n",
       " ('ache', 4),\n",
       " ('actually', 5),\n",
       " ('add', 6),\n",
       " ('afternoon', 7),\n",
       " ('ago', 8),\n",
       " ('agree', 9),\n",
       " ('ah', 10),\n",
       " ('ahh', 11),\n",
       " ('ahhh', 12),\n",
       " ('album', 13),\n",
       " ('almost', 14),\n",
       " ('already', 15),\n",
       " ('alright', 16),\n",
       " ('also', 17),\n",
       " ('always', 18),\n",
       " ('amaze', 19)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_indices = {term: index for index, term in enumerate(vector_terms)}\n",
    "\n",
    "# First 20 terms from the dictionary\n",
    "list(term_indices.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_processed</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1309492</th>\n",
       "      <td>1.0</td>\n",
       "      <td>http://twitpic.com/6ik6t - Will be doing this tomorrow.</td>\n",
       "      <td>[tomorrow]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925149</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@Rsltruly Haha, it seemed a little distraught, so I thought I'd ask</td>\n",
       "      <td>[haha, seem, little, distraught, thought, would, ask]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742849</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>@d33pak ya...but m missing my &amp;quot;beeryani&amp;quot; on the weekends</td>\n",
       "      <td>[ya, miss, beeryani, weekend]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528263</th>\n",
       "      <td>1.0</td>\n",
       "      <td>smiling. my DJ is about to hook it up for Music Monday...trade lyrics with me today too twittFAM. music IS the universal language</td>\n",
       "      <td>[smile, dj, hook, music, monday, trade, lyric, today, twittfam, music, universal, language]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393444</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>@randfish I miss the SF fog  Watching it roll in when you have nothing to do, yeah baby!</td>\n",
       "      <td>[miss, sf, fog, watch, roll, nothing, yeah_NEG, baby_NEG]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment  \\\n",
       "1309492  1.0         \n",
       "925149   1.0         \n",
       "742849  -1.0         \n",
       "1528263  1.0         \n",
       "393444  -1.0         \n",
       "\n",
       "                                                                                                                                      tweet  \\\n",
       "1309492  http://twitpic.com/6ik6t - Will be doing this tomorrow.                                                                              \n",
       "925149   @Rsltruly Haha, it seemed a little distraught, so I thought I'd ask                                                                  \n",
       "742849   @d33pak ya...but m missing my &quot;beeryani&quot; on the weekends                                                                   \n",
       "1528263  smiling. my DJ is about to hook it up for Music Monday...trade lyrics with me today too twittFAM. music IS the universal language    \n",
       "393444   @randfish I miss the SF fog  Watching it roll in when you have nothing to do, yeah baby!                                             \n",
       "\n",
       "                                                                                     tweet_processed  \\\n",
       "1309492  [tomorrow]                                                                                    \n",
       "925149   [haha, seem, little, distraught, thought, would, ask]                                         \n",
       "742849   [ya, miss, beeryani, weekend]                                                                 \n",
       "1528263  [smile, dj, hook, music, monday, trade, lyric, today, twittfam, music, universal, language]   \n",
       "393444   [miss, sf, fog, watch, roll, nothing, yeah_NEG, baby_NEG]                                     \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                    vector  \n",
       "1309492  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
       "925149   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
       "742849   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
       "1528263  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
       "393444   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function vectorizes a preprocessed text according to the term vector:\n",
    "def vectorize(terms, terms_dict=term_indices):\n",
    "    \n",
    "    # Creating a list full of zeros, according to the dictionary size\n",
    "    vector = [0] * len(terms_dict)\n",
    "    \n",
    "    # For all terms:\n",
    "    for term in terms:\n",
    "        # The vector is only updated if the term exists in our dictionary:\n",
    "        if term in terms_dict:\n",
    "            index = terms_dict[term]\n",
    "            vector[index] += 1\n",
    "        \n",
    "    return(vector)\n",
    "\n",
    "# Now we can apply this to the training and testing set:\n",
    "# dataset_train.loc[:,\"vector\"] = dataset_train[\"tweet_processed\"].apply(vectorize)\n",
    "dataset_train = dataset_train.assign(vector = dataset_train[\"tweet_processed\"].apply(vectorize))\n",
    "# dataset_test.loc[:,\"vector\"] = dataset_test[\"tweet_processed\"].apply(vectorize)\n",
    "dataset_test = dataset_test.assign(vector = dataset_test[\"tweet_processed\"].apply(vectorize))\n",
    "# You can also use map() since we are dealing with the columns here. However, apply() \n",
    "# can take multiple arguments, which is necessary if you want to pass a different \n",
    "# dictionary as terms_dict.\n",
    "\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if our vectorization works by recreating the terms from the vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector representation of the first tweet: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \n",
      "\n",
      "Unique sorted terms that can be represented for the first tweet: ['tomorrow'] \n",
      "\n",
      "All sorted terms of the first tweet: ['tomorrow']\n"
     ]
    }
   ],
   "source": [
    "print(\"Vector representation of the first tweet:\", dataset_train[\"vector\"].iloc[0], \"\\n\")\n",
    "\n",
    "print(\"Unique sorted terms that can be represented for the first tweet:\", \n",
    "      sorted([vector_terms[index] for index, term in enumerate(dataset_train[\"vector\"].iloc[0]) if term != 0]), \"\\n\")\n",
    "\n",
    "print(\"All sorted terms of the first tweet:\", sorted(dataset_train[\"tweet_processed\"].iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While these vectors are meaningless to the human eye, we can now quantify a given text and train a model using these term occurences. So, each occurrence count in the tweet vector is actually a feature. Our feature space is the vector itself.\n",
    "\n",
    "Before we move forward, there is a limitation that you should keep in mind. To dramatically reduce the vector size, we limit our bag of words to the most frequent words from the two sentiment groups. However, this elimination process may not be ideal. For example, only because a given word occurs a lot may not always mean using that word would help us with classification. It is possible for a given word to be frequent in both groups and give us no information. In this case, ignoring that word for vectorization could be actually helpful for classification. In short, we actually want words that occur frequently but not occur in every group (both positive and negative tweets in this case). We can use the term frequency-inverse document frequency ([TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)) approach to evaluate a word's usefulness in this sense. This would also inherently eliminate stop words and such that occur in both sides. We will not implement this now, but you can use it to rank these terms, remove the unimportant ones, and also normalize the vectors to obtain more efficient vectors.\n",
    "\n",
    "You can also use the [count vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and the [TF-IDF vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) of scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes<a id=\"mnb\"></a>\n",
    "\n",
    "We can use Multinomial Naive Bayes through these sentiment classes' term occurrences. Before we jump into the codes, let us go over the logic behind Mutlinomial Naive Bayes.\n",
    "\n",
    "Take a look at the ratio of tweets with a positive sentiment in our training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset.sentiment == 1].shape[0]/dataset.shape[0]\n",
    "# Remember that df.shape[0] retrieves the row count for a given DataFrame object. \n",
    "# Therefore, dataset[dataset.sentiment == 1].shape[0] retrieves the number of tweets\n",
    "# that have a sentiment value of 1 (positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means 50% of our training set has a positive sentiment. Using this information, we can roughly guess that for a given tweet, the possibility of it having a positive sentiment is 50%. This probability can be denoted by \"p(positive).\" This initial guess is called \"prior probability.\"\n",
    "\n",
    "We can improve our estimation by looking at the tweet content and comparing it with the ones we have in our dataset. For example, what is the probability of a given tweet, provided that it includes the word \"love,\" to have a positive sentiment? We had already counted the occurences of terms for each sentiment. We can use these frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0030269029042611488"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frequency of the word \"love\" divided by the total number of word occurencies in \n",
    "# negative tweets:\n",
    "terms_negative[\"love\"]/sum(terms_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010780877986142693"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frequency of the word \"love\" divided by the total number of word occurencies in \n",
    "# postive tweets:\n",
    "terms_positive[\"love\"]/sum(terms_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also think about these ratios as probabilities of finding the word \"love\" in negative and positive tweets, separately. \n",
    "\n",
    "These probabilities (or likelihoods) are shown as \"p(<span style=\"color:purple\">love</span><span style=\"color:blue\">|</span><span style=\"color:red\">negative</span>)\" and \"p(<span style=\"color:purple\">love</span><span style=\"color:blue\">|</span><span style=\"color:green\">positive</span>),\" which means <span style=\"color:purple\">the probability of \"love\"</span> <span style=\"color:blue\">given that</span> the tweet is <span style=\"color:red\">negative</span>/<span style=\"color:green\">positive</span>.\n",
    "\n",
    "Notice that the probability of finding \"love\" is much higher in positive tweets. Since the amount of positive and negative tweets are about the same in our dataset, we can intuitively understand a tweet that has the word \"love\" has a higher chance to have a positive sentiment. However, this may not be that easy with different numbers, so let us formulate it. To calculate the posterior probability (which means the probability that is calculated based on another given event/condition), we essentially multiply the prior probability with the likelihood in the sentiment group. So, the probability of a tweet to have a positive sentiment given that it has the word \"love\" (can be denoted by \"p(positive|love)\") is actually the combination of these two events:\n",
    "* The tweet has the word \"love.\"\n",
    "* The tweet is positive.\n",
    "\n",
    "Therefore, we need to multiply these separate probabilities:\n",
    "\n",
    "`p(positive|love) = p(love|positive) x p(positive)`\n",
    "\n",
    "This is the formula of Naive Bayes. If you have heard about conditional probability, it may look familiar to you. In conditional probability, we divide this multiplication by the probability of the condition happening. Since it is given that it does happen (the word \"love\" does exist), we do not include it.\n",
    "\n",
    "Now, we can calculate the exact probabilities for our case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(positive|love) = 0.0053904389930713465\n"
     ]
    }
   ],
   "source": [
    "print(\"p(positive|love) =\",(terms_positive[\"love\"]/sum(terms_positive)) * (dataset[dataset.sentiment == 1].shape[0]/dataset.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(negative|love) = 0.0015134514521305744\n"
     ]
    }
   ],
   "source": [
    "print(\"p(negative|love) =\",(terms_negative[\"love\"]/sum(terms_negative)) * (dataset[dataset.sentiment == -1].shape[0]/dataset.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if we have two conditions (for example our tweet has the words \"love\" and \"hate\"), we multiply their likelihoods along with the prior probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(negative|love ∩ hate) = 2.7398502145035067e-06\n"
     ]
    }
   ],
   "source": [
    "print(\"p(negative|love ∩ hate) =\",\n",
    "      (terms_positive[\"love\"]/sum(terms_positive)) * \n",
    "      (terms_positive[\"hate\"]/sum(terms_positive)) * \n",
    "      (dataset[dataset.sentiment == 1].shape[0]/dataset.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(negative|love ∩ hate) = 5.528878305411383e-06\n"
     ]
    }
   ],
   "source": [
    "print(\"p(negative|love ∩ hate) =\",\n",
    "      (terms_negative[\"love\"]/sum(terms_negative)) * \n",
    "      (terms_negative[\"hate\"]/sum(terms_negative)) * \n",
    "      (dataset[dataset.sentiment == -1].shape[0]/dataset.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One potential issue is that since we keep multiplying these probabilities, if our dataset does not have any occurrence of a specific word, using its likelihood would result a zero, which would absorb the whole calculation. To solve this, during calculation, we can add 1 to every frequency, so their likelihoods are never zero. However, since we represent each tweet strictly with a vector of specific words at hand, this is not a problem.\n",
    "\n",
    "Anyway, we can now jump into training a Multinomial Naive Bayes classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb_classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we have already split our dataset into training and test sets. To train a dataset, we need to split it into input and output. Our input is the tweet vector and our output is the sentiment, because we are interested in predicting the sentiment for a given tweet vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_x = dataset_train[\"vector\"].tolist()\n",
    "dataset_train_y = dataset_train[\"sentiment\"]\n",
    "\n",
    "# We can now train our model:\n",
    "mnb_classifier.fit(dataset_train_x, dataset_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now feed the training input back and see their predicted sentiments. We can compare these predictions with the expected sentiments (dataset_train_y) and evaluate the model's success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.75      0.78      0.77      5600\n",
      "         1.0       0.77      0.74      0.76      5600\n",
      "\n",
      "    accuracy                           0.76     11200\n",
      "   macro avg       0.76      0.76      0.76     11200\n",
      "weighted avg       0.76      0.76      0.76     11200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predicting the training set:\n",
    "train_predictions = mnb_classifier.predict(dataset_train_x)\n",
    "\n",
    "# Evaluating the model:\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Printing the classification report\n",
    "print(classification_report(dataset_train_y, train_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways one can evaluate the classification results. You can learn more about performance evaluation metrics from [here](https://www.kdnuggets.com/2020/04/performance-evaluation-metrics-classification.html).\n",
    "\n",
    "The results are not bad, but remember that we need our model to be successful with the unknown data, which is our testing set. Let us try to predict the testing set and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.72      0.73      0.73      2400\n",
      "         1.0       0.73      0.72      0.72      2400\n",
      "\n",
      "    accuracy                           0.72      4800\n",
      "   macro avg       0.72      0.72      0.72      4800\n",
      "weighted avg       0.72      0.72      0.72      4800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting the testing set:\n",
    "dataset_test_x = dataset_test[\"vector\"].tolist()\n",
    "dataset_test_y = dataset_test[\"sentiment\"]\n",
    "\n",
    "# Predicting the testing set:\n",
    "test_predictions = mnb_classifier.predict(dataset_test_x)\n",
    "\n",
    "# Printing the classification report\n",
    "print(classification_report(dataset_test_y, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is usually expected to see slightly lower scores with the unknown data. Still, for the amount of work, it is not that bad.\n",
    "\n",
    "We can also get the feature importances and see which words are more important to predict the sentiment. For every class, we can get the log probabilities and convert them to probabilities (for a slightly more intuitive understanding) for each word from the input vector. We can obtain them in human-readable form through `vector_terms`. Note that we did not need to train a model to see this, we could directly calculate it from the occurrences in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1st</th>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.000262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;3</th>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.003540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>able_NEG</th>\n",
       "      <td>0.000832</td>\n",
       "      <td>0.000131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absolutely</th>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ache</th>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.000087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            negative  positive\n",
       "1st         0.000745  0.000262\n",
       "<3          0.000920  0.003540\n",
       "able_NEG    0.000832  0.000131\n",
       "absolutely  0.000263  0.000655\n",
       "ache        0.000613  0.000087"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_feature_log_prob = mnb_classifier.feature_log_prob_\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# We define an empty DataFrame with the vectorm terms as indices:\n",
    "mnb_feature_prob = pd.DataFrame(index=vector_terms)\n",
    "\n",
    "# For each sentiment class:\n",
    "for sentiment_index, sentiment in enumerate(mnb_classifier.classes_):\n",
    "    # Dumping the term probabilities to mnb_feature_prob:\n",
    "    mnb_feature_prob[sentiment] = np.exp(mnb_feature_log_prob[sentiment_index])\n",
    "\n",
    "# Using floats as column names can be confusing, so we rename them in place:\n",
    "mnb_feature_prob.rename(columns={-1.0: \"negative\", 1.0: \"positive\"}, inplace=True)\n",
    "\n",
    "mnb_feature_prob.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice<a id=\"practice\"></a>\n",
    "\n",
    "Now is a good time to practice what you have learned. Use the processed forms of the tweets (\"tweet_processed\" column) and vectorize both training and testing sets **using only the top 500 positive terms**. Train the same model, predict the testing set, and evaluate the accuracy. Variables and some parts are already provided for you. You only need to fill in the lines where you see `## FILL IN HERE ##`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('<3', 0),\n",
       " ('absolutely', 1),\n",
       " ('actually', 2),\n",
       " ('add', 3),\n",
       " ('afternoon', 4),\n",
       " ('ago', 5),\n",
       " ('agree', 6),\n",
       " ('ah', 7),\n",
       " ('ahh', 8),\n",
       " ('ahhh', 9),\n",
       " ('album', 10),\n",
       " ('almost', 11),\n",
       " ('already', 12),\n",
       " ('alright', 13),\n",
       " ('also', 14),\n",
       " ('always', 15),\n",
       " ('amaze', 16),\n",
       " ('amazing', 17),\n",
       " ('another', 18),\n",
       " ('anyone', 19)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating disposable copies and dropping the existing vector column:\n",
    "practice_dataset_train = dataset_train.copy().drop(columns=\"vector\")\n",
    "practice_dataset_test = dataset_test.copy().drop(columns=\"vector\")\n",
    "\n",
    "# Obtain the positive terms that occur at least five times.\n",
    "practice_vector_terms = sorted(set(terms_positive.nlargest(500).index))\n",
    "# practice_vector_terms = ## FILL IN HERE ##\n",
    "\n",
    "# Adding the indices to the terms\n",
    "practice_term_indices = {term: index for index, term in enumerate(practice_vector_terms)}\n",
    "\n",
    "# Vector size (this must be 500):\n",
    "print(\"Vector size:\",len(practice_vector_terms))\n",
    "\n",
    "# First 20 terms from the dictionary\n",
    "list(practice_term_indices.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the terms you obtained to vectorize the sets.\n",
    "# Hint: Do not forget to pass your new term indices while applying vectorize.\n",
    "# Otherwise, you will obtain the same vectors by default.\n",
    "practice_dataset_train[\"vector\"] = practice_dataset_train[\"tweet_processed\"].apply(vectorize, terms_dict=practice_term_indices)\n",
    "practice_dataset_test[\"vector\"] = practice_dataset_test[\"tweet_processed\"].apply(vectorize, terms_dict=practice_term_indices)\n",
    "# practice_dataset_train[\"vector\"] = ## FILL IN HERE ##\n",
    "# practice_dataset_test[\"vector\"] = ## FILL IN HERE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.69      0.74      0.72      2400\n",
      "         1.0       0.72      0.67      0.70      2400\n",
      "\n",
      "    accuracy                           0.71      4800\n",
      "   macro avg       0.71      0.71      0.71      4800\n",
      "weighted avg       0.71      0.71      0.71      4800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Obtain the input and output for training.\n",
    "practice_dataset_train_x = practice_dataset_train[\"vector\"].tolist()\n",
    "practice_dataset_train_y = practice_dataset_train[\"sentiment\"]\n",
    "# practice_dataset_train_x = ## FILL IN HERE ##\n",
    "# practice_dataset_train_y = ## FILL IN HERE ##\n",
    "\n",
    "# Obtain the input and output for testing.\n",
    "practice_dataset_test_x = practice_dataset_test[\"vector\"].tolist()\n",
    "practice_dataset_test_y = practice_dataset_test[\"sentiment\"]\n",
    "# practice_dataset_test_x = ## FILL IN HERE ##\n",
    "# practice_dataset_test_y = ## FILL IN HERE ##\n",
    "\n",
    "# Training the model:\n",
    "mnb_classifier.fit(practice_dataset_train_x, practice_dataset_train_y)\n",
    "\n",
    "# Predicting the testing set:\n",
    "practice_test_predictions = mnb_classifier.predict(practice_dataset_test_x)\n",
    "\n",
    "# Printing the classification report\n",
    "print(classification_report(practice_dataset_test_y, practice_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results you obtained with the original one. How and why are they different? Feel free to experiment with the actual code as well. Reduce `frac_to_drop` to use more of the dataset, change vector size, process the tweets further, etc. and check the difference they make.\n",
    "\n",
    "## Using another model<a id=\"svm\"></a>\n",
    "\n",
    "Once we obtain the vector representations, we can switch the model. For example, instead of Multinomial Naive Bayes, we can use Support Vector Machine (SVM). While the codes we will use are very similar to Multinomial Naive Bayes thanks to the library, SVM has a very different working principle. You can watch [this short video](https://www.youtube.com/watch?v=Y6RRHw9uN9o) to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.75      0.68      0.71      2400\n",
      "         1.0       0.70      0.77      0.74      2400\n",
      "\n",
      "    accuracy                           0.72      4800\n",
      "   macro avg       0.73      0.72      0.72      4800\n",
      "weighted avg       0.73      0.72      0.72      4800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Defining an SVM model:\n",
    "svm_classifier = svm.SVC()\n",
    "\n",
    "# Training the model:\n",
    "svm_classifier.fit(dataset_train_x, dataset_train_y)\n",
    "\n",
    "# Predicting the testing set:\n",
    "test_predictions = svm_classifier.predict(dataset_test_x)\n",
    "\n",
    "# Printing the classification report\n",
    "print(classification_report(dataset_test_y, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A better way to vectorize<a id=\"vectorize-better\"></a>\n",
    "\n",
    "The way we vectorize the tweets is easy to understand but a bit manual labor. We could write a function that handles this for us, but it is also a bit primitive. There are more sophisticated alternatives. For example, we can use Word2Vec or Doc2Vec neural network models to vectorize a word or a document (which can be a sentence, a paragraph, or even an actual text document in this context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# Obtaining TaggedDocument objects in a list using the training set:\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(dataset_train[\"tweet_processed\"])]\n",
    "\n",
    "# Training Doc2Vec model using these documents:\n",
    "model = Doc2Vec(documents, vector_size=2, window=2, min_count=1, workers=4)\n",
    "# vector_size: Number of dimensions that will represent our tweets\n",
    "# window: Simply put, the window size that will be used to detect occurrence patterns\n",
    "# min_count: A minimum threshold to filter out less common words\n",
    "# workers: Number of concurrent threads to parallelize the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now obtain the vector that represents a document (more like a list of words). Take a look at this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.18386315, -0.12003087], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtaining the vector\n",
    "model.infer_vector([\"natural\", \"language\", \"processing\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, take a look at this one and compare their vector representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.14088036, -0.00049924], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector([\"processing\", \"language\", \"natural\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have three important questions in your mind now:\n",
    "1. **Why do we get these weird numbers?**\n",
    "\n",
    "Previously, what we had obtained was a simple bag of words. So, each word's occurrence (given that they are chosen to form our vector) was simply represented by one component of the vector. Here, each component is actually a dimension rather than a mere occurrence count of a given word in that document. This is called \"distributed representation.\" Think about the furnitures and items in your room. They all have an X, Y, and Z coordinates with respect to the center point of your room. Therefore, we can represent these furnitures using `<X, Y, Z>` vectors. The lamp is right at the center of your ceiling. The carpet is at the floor. The wall clock is on the wall in front of you, vertically between the lamp and the carpet. X, Y, or Z do not represent the existence of a specific furniture. These furnitures exist at those specific points in the space (your room). So, it is possible to represent words or documents in high-dimensional spaces.\n",
    "\n",
    "1. **Can we even represent many words (documents) with smaller vector sizes?**\n",
    "\n",
    "Think about the amount of furnitures you can represent using their relative X, Y, and Z coordinates in your room. It is way more than the number of dimenions in this scenario. Likewise, we can now efficiently squeeze in many representations in fewer dimensions. Still, this example vectorization was overly simplified for demonstration purposes. Normally, higher dimensions (something between 100–⁠300) are preferred. Compared to the simple bag of words approach, it is slightly harder to comprehend and envision higher dimensions. Luckily, computers do not care as much.\n",
    "\n",
    "1. **Why do not these two documents have the same representation?**\n",
    "\n",
    "Remember that we are now using Doc2Vec and we embed documents. So, instead of representing furnitures in your room, we are actually representing the room itself. When we change the position of a certain furniture (word) in the room (document), the representation changes. In some cases, you might prefer to use Word2Vec, obtain word-level vectors, and average them together to obtain document-level representations.\n",
    "\n",
    "Let us vectorize the dataset using Doc2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.1857041e-03, -1.1108252e-04,  9.1356365e-03, -5.4456051e-03,\n",
       "        3.6530979e-03,  3.9595556e-03,  5.9361104e-03,  9.1059942e-04,\n",
       "        4.4397335e-03,  2.9931783e-03, -1.6850706e-03, -7.4246032e-03,\n",
       "       -8.9147817e-03,  7.4067377e-03,  1.1401853e-02,  8.0902474e-03,\n",
       "       -4.8305144e-04, -2.3091985e-03, -3.5639931e-03, -2.7307381e-03,\n",
       "        3.8128106e-03, -1.9912468e-03,  2.2229594e-03,  5.2613528e-03,\n",
       "       -4.9986602e-03,  1.3323586e-03, -5.9751363e-04,  9.3461610e-03,\n",
       "        2.4555272e-03,  5.1593720e-03, -7.3330291e-04, -7.5872143e-05,\n",
       "        2.4905060e-03, -6.3853473e-03, -7.4300072e-03, -9.6328920e-03,\n",
       "        4.8524430e-03,  1.7707457e-03,  3.8407745e-03, -1.8493695e-03,\n",
       "        8.4763467e-03, -9.4923383e-04,  4.7179200e-03, -3.4872659e-03,\n",
       "       -4.7378466e-03, -3.0119510e-03,  5.3248536e-03, -3.9622621e-03,\n",
       "        7.2209854e-03, -2.2473580e-03, -3.7954634e-03,  9.5955208e-03,\n",
       "       -3.2739752e-04, -3.7175650e-03,  3.1177369e-03, -7.6276860e-03,\n",
       "        8.0953017e-03, -2.0806610e-03, -9.1250949e-03,  8.2067156e-04,\n",
       "        1.0967837e-02, -4.5331032e-03,  2.0633559e-03,  3.4459119e-03,\n",
       "       -5.5591133e-03,  9.9550020e-03, -6.1169211e-03,  8.1844162e-04,\n",
       "       -1.2033619e-02,  6.2669544e-03,  2.6980492e-03, -6.3080578e-03,\n",
       "       -1.5182607e-03, -5.1296530e-03,  5.1808297e-03,  9.3287490e-03,\n",
       "        7.6022218e-03,  2.8977925e-05, -5.4552080e-04, -2.8599843e-03,\n",
       "        1.1822768e-02, -5.2445401e-03,  3.9381227e-03, -9.4499446e-05,\n",
       "        4.8357332e-03, -1.3462978e-02, -1.6644119e-03,  4.6475977e-03,\n",
       "       -2.7203411e-03, -4.9541495e-03, -3.6406461e-03,  4.0593417e-04,\n",
       "        1.0317810e-02, -1.8192094e-03, -1.1087676e-04, -8.1441388e-04,\n",
       "        6.4555649e-03,  4.7350517e-03, -4.7237771e-03, -9.6441191e-03,\n",
       "       -5.4284008e-03, -3.5403511e-03, -1.4855942e-03,  6.1513404e-03,\n",
       "        6.7963824e-04,  3.1147317e-03, -3.7123964e-03,  1.3485454e-02,\n",
       "        1.3653570e-03,  9.2102466e-03,  1.3435936e-02, -6.0179192e-03,\n",
       "       -1.2260680e-03, -3.1790693e-04, -4.1283905e-03,  6.9024172e-03,\n",
       "        7.9456354e-03,  8.4581738e-03,  5.4134950e-03,  7.8954995e-03,\n",
       "       -2.8476720e-03, -8.6907791e-03,  2.0514640e-03,  3.8306778e-03,\n",
       "        3.4778812e-03, -2.2806013e-03,  2.3486279e-03, -3.6104780e-03,\n",
       "       -3.5409092e-03, -8.0582674e-04,  5.9098783e-03,  4.3860972e-03,\n",
       "        6.7312173e-03,  6.9731059e-03,  5.2786828e-03,  6.3451019e-04,\n",
       "        1.1693469e-03, -7.5361930e-04,  5.9053237e-03,  6.5078987e-03,\n",
       "       -1.0746051e-04,  9.8340577e-03, -3.2636095e-03, -2.4097983e-03,\n",
       "       -6.6631455e-03, -4.2718630e-03, -5.6359335e-03, -6.8005971e-03,\n",
       "        4.8075560e-03, -4.0931180e-03, -5.8779883e-04,  7.6082190e-03,\n",
       "        7.0779228e-05, -1.2756153e-02,  4.0254532e-03, -2.8739404e-03,\n",
       "        9.1228960e-03, -4.4949614e-03,  7.1856198e-03,  2.5019771e-03,\n",
       "        5.0299913e-03,  9.0246042e-03,  6.2791128e-03,  4.9532270e-03,\n",
       "       -9.1110738e-03,  1.6704898e-03,  1.4804219e-03,  3.5732444e-03,\n",
       "        8.3996207e-03, -5.6010231e-06,  6.3460269e-03,  1.9605127e-03,\n",
       "        5.0445329e-03, -1.3522222e-03, -8.7005319e-03,  1.1033733e-02,\n",
       "        5.6180628e-03,  3.9271000e-03, -1.0684751e-02, -1.2476220e-02,\n",
       "       -1.5066073e-03,  3.2753781e-03,  9.7621577e-03,  1.4395103e-02,\n",
       "       -3.9710957e-03, -8.1500271e-03,  1.0321652e-02, -9.7094337e-03,\n",
       "        2.3364441e-03, -3.4076013e-03,  4.8746932e-03,  8.0087492e-03,\n",
       "        6.6397795e-03, -2.6724129e-03,  8.5666068e-03,  3.2067755e-03,\n",
       "        9.7262271e-04,  2.0836445e-03, -2.5490415e-03,  1.1966218e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us firstly train a proper Doc2Vec model:\n",
    "model = Doc2Vec(documents, vector_size=200, workers=4)\n",
    "\n",
    "# Vectorization with Doc2Vec:\n",
    "dataset_train_x_distributed = [model.infer_vector(tweet_processed) for tweet_processed in dataset_train[\"tweet_processed\"]]\n",
    "dataset_test_x_distributed = [model.infer_vector(tweet_processed) for tweet_processed in dataset_test[\"tweet_processed\"]]\n",
    "\n",
    "# Distributed representation for the first row:\n",
    "dataset_train_x_distributed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not have frequencies with a distributed representation, we cannot use Multinomial Naive Bayes anymore. Instead, we can use a model that can work with continuous data. Here, we will use Gaussian Naive Bayes, but it would be possible to use SVM or another model as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.54      0.25      0.34      2400\n",
      "         1.0       0.51      0.79      0.62      2400\n",
      "\n",
      "    accuracy                           0.52      4800\n",
      "   macro avg       0.52      0.52      0.48      4800\n",
      "weighted avg       0.52      0.52      0.48      4800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Defining an SVM model:\n",
    "gnb_classifier = GaussianNB()\n",
    "\n",
    "# Training the model:\n",
    "gnb_classifier.fit(dataset_train_x_distributed, dataset_train_y)\n",
    "\n",
    "# Predicting the testing set:\n",
    "test_predictions = gnb_classifier.predict(dataset_test_x_distributed)\n",
    "\n",
    "# Printing the classification report\n",
    "print(classification_report(dataset_test_y, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a vector size of 200 and default parameters, it does not work well, which is not surprising. Note that, since these are neural networks models, they require a lot of data. You can try training it with using a bigger portion of the whole dataset. Instead of finding a large training set and time to train it, it is common to use pre-trained [GloVe](https://nlp.stanford.edu/projects/glove/) models for vectorization. You will learn more about these later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More information<a id=\"more-info\"></a>\n",
    "\n",
    "* [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "* [scikit-learn](https://scikit-learn.org)\n",
    "    * [Count vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "    * [TF-IDF vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "    * [Multinomial Naive Bayes classifier](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "    * [SVM classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "    * [Gaussian Naive Bayes classifier](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n",
    "* [Gensim](https://radimrehurek.com/gensim/)\n",
    "    * [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "    * [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)\n",
    "* [GloVe](https://nlp.stanford.edu/projects/glove/)"
   ]
  }
 ],
 "metadata": {
  "author": "Özgün Ozan Kılıç",
  "institute": "Informatics Institute, Middle East Technical University",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
